{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbgW6bMxYJnq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_TSV  = \"/content/sample_data/Tagged_Titles_Train.tsv\"\n",
        "OUT_DIR    = \"/content/sample_data/eBay_ML_Challenge_2025/conll\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "NgsP-bPSZU8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\n",
        "    INPUT_TSV,\n",
        "    sep='\\t',\n",
        "    header=0,\n",
        "    dtype=str,\n",
        "    na_values=[''],   # only these become NaN\n",
        "    keep_default_na=False\n",
        ")\n",
        "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "df['tag'] = df['tag'].fillna('Blank')"
      ],
      "metadata": {
        "id": "iJooGvrbZmph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = os.path.join(OUT_DIR, f\"train_data.conll\")\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for (_, _, title), group in df.groupby([\"record_number\",\"category\",\"title\"], sort=False):\n",
        "            for tok, tg in zip(group[\"token\"], group[\"tag\"]):\n",
        "                fout.write(f\"{tok} {tg}\\n\")\n",
        "            fout.write(\"\\n\")\n",
        "    print(f\"Wrote {len(df['record_number'].unique())} sentences to {out_path}\")"
      ],
      "metadata": {
        "id": "yiziVb0UZpjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "def conll_to_spacy_every_token(input_path: str, output_path: str, lang: str = \"de\"):\n",
        "    \"\"\"\n",
        "    Convert a space-separated .conll of `token LABEL` into spaCy's binary DocBin format,\n",
        "    treating each token as a one-token entity. Fixes the TypeError by tracking document count.\n",
        "    \"\"\"\n",
        "    # Initialize blank pipeline and DocBin\n",
        "    nlp = spacy.blank(lang)\n",
        "    doc_bin = DocBin()\n",
        "    doc_count = 0\n",
        "    tokens, labels = [], []\n",
        "\n",
        "    def flush_sentence():\n",
        "        nonlocal doc_count\n",
        "        if not tokens:\n",
        "            return\n",
        "        # Reconstruct text and create Doc\n",
        "        doc = nlp.make_doc(\" \".join(tokens))\n",
        "        spans = []\n",
        "        char_offset = 0\n",
        "        # Create one-token spans for each label\n",
        "        for token_text, label in zip(tokens, labels):\n",
        "            start = char_offset\n",
        "            end = start + len(token_text)\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "            if span:\n",
        "                spans.append(span)\n",
        "            char_offset = end + 1  # account for space\n",
        "        doc.ents = spans\n",
        "        doc_bin.add(doc)\n",
        "        doc_count += 1\n",
        "        tokens.clear()\n",
        "        labels.clear()\n",
        "\n",
        "    # Read and process the .conll file\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            stripped = line.strip()\n",
        "            if not stripped:\n",
        "                # Sentence boundary: flush collected tokens\n",
        "                flush_sentence()\n",
        "            else:\n",
        "                parts = stripped.split()\n",
        "                token = parts[0]\n",
        "                label = parts[1]\n",
        "                tokens.append(token)\n",
        "                labels.append(label)\n",
        "    # Flush any remaining tokens\n",
        "    flush_sentence()\n",
        "\n",
        "    # Save to disk\n",
        "    doc_bin.to_disk(output_path)\n",
        "    print(f\"✅ Saved {doc_count} documents to {output_path}\")\n",
        "\n",
        "conll_to_spacy_every_token(\n",
        "    \"/content/sample_data/eBay_ML_Challenge_2025/conll/train_data.conll\",\n",
        "    \"/content/sample_data/eBay_ML_Challenge_2025/conll/train_data.spacy\"\n",
        ")"
      ],
      "metadata": {
        "id": "VXu93QU4Z4li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy-transformers"
      ],
      "metadata": {
        "id": "daafdc5ibA3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/sample_data/config.cfg --output \"/content/sample_data/eBay_ML_Challenge_2025/roberta/\" --paths.train \"/content/sample_data/eBay_ML_Challenge_2025/conll/train_data.spacy\" --paths.dev \"/content/sample_data/eBay_ML_Challenge_2025/conll/train_data.spacy\" --gpu-id 0"
      ],
      "metadata": {
        "id": "ZaWyLqIPbu59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_TSV  = \"/content/sample_data/Listing_Titles.tsv\"\n",
        "START_ROW  = 5001\n",
        "END_ROW    = 30000\n",
        "FILTERED_DATA = \"/content/sample_data/filtered_data.tsv\"\n",
        "MODEL = \"/content/sample_data/eBay_ML_Challenge_2025/roberta/model-best\"\n",
        "OUTPUT_TSV  = \"/content/sample_data/predictions.tsv\"\n",
        "OUTPUT_COMB = \"/content/sample_data/predictions_combined.tsv\"\n",
        "OUTPUT_SWAP = \"/content/sample_data/final_predictions.tsv\""
      ],
      "metadata": {
        "id": "CGCakCOqoKPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tJMRDE2xoz3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1) skiprows drops lines 1…(START_ROW-1) after the header (line 0)\n",
        "skiprows = list(range(1, START_ROW))\n",
        "# 2) nrows = number of lines from START_ROW through END_ROW, inclusive\n",
        "nrows = END_ROW - START_ROW + 1\n",
        "\n",
        "df = pd.read_csv(\n",
        "    INPUT_TSV,\n",
        "    sep=\"\\t\",\n",
        "    header=0,\n",
        "    skiprows=skiprows,\n",
        "    nrows=nrows,\n",
        "    names=[\"record_number\",\"category\",\"title\"],\n",
        "    encoding=\"utf-8\",\n",
        "    engine=\"python\",\n",
        ")\n",
        "df.to_csv(FILTERED_DATA, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "3sHJpkajZ6di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy_transformers import Transformer\n",
        "# 1) Enable GPU\n",
        "spacy.require_gpu(0)\n",
        "# 2) Load model\n",
        "nlp = spacy.load(MODEL)"
      ],
      "metadata": {
        "id": "EQFRjdrFpgCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "results = []\n",
        "\n",
        "label_filters = {\n",
        "    1: {\"label_cat1\"},\n",
        "    2: {\"label_cat2\"},\n",
        "}\n",
        "\n",
        "for rec, cat, title in tqdm(\n",
        "    zip(df.record_number, df.category, df.title),\n",
        "    total=len(df),\n",
        "    desc=\"Predicting entities\"\n",
        "):\n",
        "    doc = nlp(title)\n",
        "    for ent in doc.ents:\n",
        "        # Check if entity label matches the allowed labels for this category\n",
        "        if ent.label_ in label_filters.get(cat, set()):\n",
        "            results.append([rec, cat, ent.text, ent.label_])"
      ],
      "metadata": {
        "id": "Z4MNKxXbprvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Save predictions\n",
        "out_df = pd.DataFrame(results, columns=[\"record_number\",\"category\",\"entity\",\"tag\"])\n",
        "out_df.to_csv(OUTPUT_TSV, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\")\n",
        "print(f\"✅ Saved {len(out_df)} predictions to {OUTPUT_TSV}\")"
      ],
      "metadata": {
        "id": "1dZJ-Mgtp47D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Load your raw predictions (no header, two columns: tag, label)\n",
        "df = pd.read_csv(\n",
        "    OUTPUT_TSV,\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"record_number\", \"category\", \"tag\", \"label\"],\n",
        "    dtype={\"record_number\": str, \"category\": str, \"tag\": str, \"label\": str},\n",
        "    encoding=\"utf-8\",\n",
        "    keep_default_na=False\n",
        ")\n",
        "\n",
        "# 2) Combine Blank‐tag rows into the previous entity\n",
        "combined = []\n",
        "for rec, cat, lbl, tg in df.itertuples(index=False):\n",
        "    lbl = str(lbl).strip()\n",
        "\n",
        "    # Skip 'O' tags\n",
        "    #if tg == \"O\":\n",
        "    #    continue\n",
        "\n",
        "    # Combine with previous if tag is \"Blank\"\n",
        "    if tg == \"Blank\" and combined:\n",
        "        combined[-1][2] += \" \" + lbl\n",
        "    else:\n",
        "        combined.append([rec, cat, lbl, tg])\n",
        "\n",
        "# 3) Write out, preserving record_number & category, no header\n",
        "out = pd.DataFrame(combined, columns=[\"record_number\", \"category\", \"label\", \"tag\"])\n",
        "out.to_csv(\n",
        "    OUTPUT_COMB,\n",
        "    sep=\"\\t\",\n",
        "    index=False,\n",
        "    header=False,\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "print(f\"Wrote {len(out)} merged predictions to preds_combined_with_rc.tsv\")"
      ],
      "metadata": {
        "id": "cZakJAuwp5cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(OUTPUT_COMB, sep=\"\\t\", header=None, names=[\"rec\",\"cat\",\"lbl\",\"tg\"], dtype=str, keep_default_na=False)\n",
        "\n",
        "# 2. Reorder columns: swap lbl and tg\n",
        "df = df[[\"rec\",\"cat\",\"tg\",\"lbl\"]]\n",
        "\n",
        "# 3. Write back out as TSV (no header)\n",
        "df.to_csv(OUTPUT_SWAP, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "FhKK2cuep7Ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}